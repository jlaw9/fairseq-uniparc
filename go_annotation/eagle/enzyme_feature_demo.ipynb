{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook to extract trained feature vectors from enzyme sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if we're using a GPU\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a bit of a pain, we need to have the layers I used to finetune the model in the python path\n",
    "# for the roberta loading to work\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from go_annotation.ontology import Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.models.roberta import RobertaModel\n",
    "\n",
    "roberta = RobertaModel.from_pretrained(\n",
    "    '/projects/deepgreen/pstjohn/20210121_go_checkpoints/',\n",
    "    data_name_or_path='/projects/deepgreen/pstjohn/swissprot_go_annotation/fairseq_swissprot/',\n",
    "    checkpoint_file='swissprot_preinit.pt')\n",
    "\n",
    "_ = roberta.eval()  # disable dropout\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "_ = roberta.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('model.encoder.sentence_encoder.embed_tokens.weight', 25344),\n",
       "  ('model.encoder.sentence_encoder.embed_positions.weight', 787968),\n",
       "  ('model.encoder.sentence_encoder.layers.0.self_attn.k_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.0.self_attn.k_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.0.self_attn.v_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.0.self_attn.v_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.0.self_attn.q_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.0.self_attn.q_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.0.self_attn.out_proj.weight',\n",
       "   589824),\n",
       "  ('model.encoder.sentence_encoder.layers.0.self_attn.out_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.0.self_attn_layer_norm.weight', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.0.self_attn_layer_norm.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.0.fc1.weight', 2359296),\n",
       "  ('model.encoder.sentence_encoder.layers.0.fc1.bias', 3072),\n",
       "  ('model.encoder.sentence_encoder.layers.0.fc2.weight', 2359296),\n",
       "  ('model.encoder.sentence_encoder.layers.0.fc2.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.0.final_layer_norm.weight', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.0.final_layer_norm.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.1.self_attn.k_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.1.self_attn.k_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.1.self_attn.v_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.1.self_attn.v_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.1.self_attn.q_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.1.self_attn.q_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.1.self_attn.out_proj.weight',\n",
       "   589824),\n",
       "  ('model.encoder.sentence_encoder.layers.1.self_attn.out_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.1.self_attn_layer_norm.weight', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.1.self_attn_layer_norm.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.1.fc1.weight', 2359296),\n",
       "  ('model.encoder.sentence_encoder.layers.1.fc1.bias', 3072),\n",
       "  ('model.encoder.sentence_encoder.layers.1.fc2.weight', 2359296),\n",
       "  ('model.encoder.sentence_encoder.layers.1.fc2.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.1.final_layer_norm.weight', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.1.final_layer_norm.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.2.self_attn.k_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.2.self_attn.k_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.2.self_attn.v_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.2.self_attn.v_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.2.self_attn.q_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.2.self_attn.q_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.2.self_attn.out_proj.weight',\n",
       "   589824),\n",
       "  ('model.encoder.sentence_encoder.layers.2.self_attn.out_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.2.self_attn_layer_norm.weight', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.2.self_attn_layer_norm.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.2.fc1.weight', 2359296),\n",
       "  ('model.encoder.sentence_encoder.layers.2.fc1.bias', 3072),\n",
       "  ('model.encoder.sentence_encoder.layers.2.fc2.weight', 2359296),\n",
       "  ('model.encoder.sentence_encoder.layers.2.fc2.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.2.final_layer_norm.weight', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.2.final_layer_norm.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.3.self_attn.k_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.3.self_attn.k_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.3.self_attn.v_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.3.self_attn.v_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.3.self_attn.q_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.3.self_attn.q_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.3.self_attn.out_proj.weight',\n",
       "   589824),\n",
       "  ('model.encoder.sentence_encoder.layers.3.self_attn.out_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.3.self_attn_layer_norm.weight', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.3.self_attn_layer_norm.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.3.fc1.weight', 2359296),\n",
       "  ('model.encoder.sentence_encoder.layers.3.fc1.bias', 3072),\n",
       "  ('model.encoder.sentence_encoder.layers.3.fc2.weight', 2359296),\n",
       "  ('model.encoder.sentence_encoder.layers.3.fc2.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.3.final_layer_norm.weight', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.3.final_layer_norm.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.4.self_attn.k_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.4.self_attn.k_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.4.self_attn.v_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.4.self_attn.v_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.4.self_attn.q_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.4.self_attn.q_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.4.self_attn.out_proj.weight',\n",
       "   589824),\n",
       "  ('model.encoder.sentence_encoder.layers.4.self_attn.out_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.4.self_attn_layer_norm.weight', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.4.self_attn_layer_norm.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.4.fc1.weight', 2359296),\n",
       "  ('model.encoder.sentence_encoder.layers.4.fc1.bias', 3072),\n",
       "  ('model.encoder.sentence_encoder.layers.4.fc2.weight', 2359296),\n",
       "  ('model.encoder.sentence_encoder.layers.4.fc2.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.4.final_layer_norm.weight', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.4.final_layer_norm.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.5.self_attn.k_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.5.self_attn.k_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.5.self_attn.v_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.5.self_attn.v_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.5.self_attn.q_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.5.self_attn.q_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.5.self_attn.out_proj.weight',\n",
       "   589824),\n",
       "  ('model.encoder.sentence_encoder.layers.5.self_attn.out_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.5.self_attn_layer_norm.weight', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.5.self_attn_layer_norm.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.5.fc1.weight', 2359296),\n",
       "  ('model.encoder.sentence_encoder.layers.5.fc1.bias', 3072),\n",
       "  ('model.encoder.sentence_encoder.layers.5.fc2.weight', 2359296),\n",
       "  ('model.encoder.sentence_encoder.layers.5.fc2.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.5.final_layer_norm.weight', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.5.final_layer_norm.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.6.self_attn.k_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.6.self_attn.k_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.6.self_attn.v_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.6.self_attn.v_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.6.self_attn.q_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.6.self_attn.q_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.6.self_attn.out_proj.weight',\n",
       "   589824),\n",
       "  ('model.encoder.sentence_encoder.layers.6.self_attn.out_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.6.self_attn_layer_norm.weight', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.6.self_attn_layer_norm.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.6.fc1.weight', 2359296),\n",
       "  ('model.encoder.sentence_encoder.layers.6.fc1.bias', 3072),\n",
       "  ('model.encoder.sentence_encoder.layers.6.fc2.weight', 2359296),\n",
       "  ('model.encoder.sentence_encoder.layers.6.fc2.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.6.final_layer_norm.weight', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.6.final_layer_norm.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.7.self_attn.k_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.7.self_attn.k_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.7.self_attn.v_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.7.self_attn.v_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.7.self_attn.q_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.7.self_attn.q_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.7.self_attn.out_proj.weight',\n",
       "   589824),\n",
       "  ('model.encoder.sentence_encoder.layers.7.self_attn.out_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.7.self_attn_layer_norm.weight', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.7.self_attn_layer_norm.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.7.fc1.weight', 2359296),\n",
       "  ('model.encoder.sentence_encoder.layers.7.fc1.bias', 3072),\n",
       "  ('model.encoder.sentence_encoder.layers.7.fc2.weight', 2359296),\n",
       "  ('model.encoder.sentence_encoder.layers.7.fc2.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.7.final_layer_norm.weight', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.7.final_layer_norm.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.8.self_attn.k_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.8.self_attn.k_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.8.self_attn.v_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.8.self_attn.v_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.8.self_attn.q_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.8.self_attn.q_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.8.self_attn.out_proj.weight',\n",
       "   589824),\n",
       "  ('model.encoder.sentence_encoder.layers.8.self_attn.out_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.8.self_attn_layer_norm.weight', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.8.self_attn_layer_norm.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.8.fc1.weight', 2359296),\n",
       "  ('model.encoder.sentence_encoder.layers.8.fc1.bias', 3072),\n",
       "  ('model.encoder.sentence_encoder.layers.8.fc2.weight', 2359296),\n",
       "  ('model.encoder.sentence_encoder.layers.8.fc2.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.8.final_layer_norm.weight', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.8.final_layer_norm.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.9.self_attn.k_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.9.self_attn.k_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.9.self_attn.v_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.9.self_attn.v_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.9.self_attn.q_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.9.self_attn.q_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.9.self_attn.out_proj.weight',\n",
       "   589824),\n",
       "  ('model.encoder.sentence_encoder.layers.9.self_attn.out_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.9.self_attn_layer_norm.weight', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.9.self_attn_layer_norm.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.9.fc1.weight', 2359296),\n",
       "  ('model.encoder.sentence_encoder.layers.9.fc1.bias', 3072),\n",
       "  ('model.encoder.sentence_encoder.layers.9.fc2.weight', 2359296),\n",
       "  ('model.encoder.sentence_encoder.layers.9.fc2.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.9.final_layer_norm.weight', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.9.final_layer_norm.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.10.self_attn.k_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.10.self_attn.k_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.10.self_attn.v_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.10.self_attn.v_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.10.self_attn.q_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.10.self_attn.q_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.10.self_attn.out_proj.weight',\n",
       "   589824),\n",
       "  ('model.encoder.sentence_encoder.layers.10.self_attn.out_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.10.self_attn_layer_norm.weight',\n",
       "   768),\n",
       "  ('model.encoder.sentence_encoder.layers.10.self_attn_layer_norm.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.10.fc1.weight', 2359296),\n",
       "  ('model.encoder.sentence_encoder.layers.10.fc1.bias', 3072),\n",
       "  ('model.encoder.sentence_encoder.layers.10.fc2.weight', 2359296),\n",
       "  ('model.encoder.sentence_encoder.layers.10.fc2.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.10.final_layer_norm.weight', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.10.final_layer_norm.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.11.self_attn.k_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.11.self_attn.k_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.11.self_attn.v_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.11.self_attn.v_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.11.self_attn.q_proj.weight', 589824),\n",
       "  ('model.encoder.sentence_encoder.layers.11.self_attn.q_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.11.self_attn.out_proj.weight',\n",
       "   589824),\n",
       "  ('model.encoder.sentence_encoder.layers.11.self_attn.out_proj.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.11.self_attn_layer_norm.weight',\n",
       "   768),\n",
       "  ('model.encoder.sentence_encoder.layers.11.self_attn_layer_norm.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.11.fc1.weight', 2359296),\n",
       "  ('model.encoder.sentence_encoder.layers.11.fc1.bias', 3072),\n",
       "  ('model.encoder.sentence_encoder.layers.11.fc2.weight', 2359296),\n",
       "  ('model.encoder.sentence_encoder.layers.11.fc2.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.11.final_layer_norm.weight', 768),\n",
       "  ('model.encoder.sentence_encoder.layers.11.final_layer_norm.bias', 768),\n",
       "  ('model.encoder.sentence_encoder.emb_layer_norm.weight', 768),\n",
       "  ('model.encoder.sentence_encoder.emb_layer_norm.bias', 768),\n",
       "  ('model.encoder.lm_head.bias', 33),\n",
       "  ('model.encoder.lm_head.dense.weight', 589824),\n",
       "  ('model.encoder.lm_head.dense.bias', 768),\n",
       "  ('model.encoder.lm_head.layer_norm.weight', 768),\n",
       "  ('model.encoder.lm_head.layer_norm.bias', 768),\n",
       "  ('model.classification_heads.go_prediction.dense.weight', 589824),\n",
       "  ('model.classification_heads.go_prediction.dense.bias', 768),\n",
       "  ('model.classification_heads.go_prediction.out_proj.weight', 24585216),\n",
       "  ('model.classification_heads.go_prediction.out_proj.bias', 32012)],\n",
       " 111669293)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    table = []\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table += [(name, param)]\n",
    "        total_params+=param\n",
    "\n",
    "    return table, total_params\n",
    "    \n",
    "count_parameters(roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 20,  8, 15, 14, 21,  8,  9,  5,  6, 11,  5, 17, 12, 16, 11, 16, 16,\n",
       "         4, 21,  5,  5, 20,  5, 13, 11, 17,  4,  9, 21, 20, 23, 10,  4, 13, 12,\n",
       "        13,  8, 14, 14, 12, 11,  5, 10, 18, 11,  6, 12, 12, 23, 11, 12,  6, 14,\n",
       "         5,  8, 10,  8,  7,  9, 11,  4, 15,  9, 20, 12, 15,  8,  6, 20, 18,  7,\n",
       "         5, 10,  4, 18, 17,  8, 21,  6, 11, 21,  9, 19, 21,  5,  9, 11, 12, 15,\n",
       "        18,  7, 10, 11,  5, 11,  9,  8, 17,  5,  8, 13, 14, 12,  4, 19, 10, 14,\n",
       "         7,  5,  7,  5,  4, 13, 11, 15,  6, 14,  9, 12, 10, 11,  6,  4, 12, 15,\n",
       "         6,  8,  6, 11,  5,  9,  7,  9,  4, 15, 15,  6,  5, 11,  4, 15, 12, 11,\n",
       "         4, 13, 18,  5, 19, 20,  9, 15, 23, 13,  9, 18, 12,  4, 22,  4, 13, 19,\n",
       "        15, 18, 12, 23, 15,  7,  7,  9,  7,  6,  8, 15, 12, 19,  7, 13, 13,  6,\n",
       "         4, 12,  8,  4, 16,  7, 15, 16, 15,  6,  5, 13, 17,  4,  7, 11,  9,  7,\n",
       "         9, 18,  6,  6,  8,  4,  6,  8, 15, 15,  6,  7, 18,  4, 14,  6,  5,  5,\n",
       "         7, 13,  4, 14,  5,  7,  8,  9, 15, 13, 12, 16, 13,  4, 15, 17,  6,  7,\n",
       "         9, 16, 13,  7, 13, 20,  7, 17,  5,  8, 17, 12, 10, 15,  5,  8, 13,  7,\n",
       "        21,  9,  7, 10, 15,  7,  4,  6,  9, 15,  6, 15, 18, 12, 15, 12, 12,  8,\n",
       "        15, 12,  9, 18, 21,  9,  6,  7, 10, 10, 17, 13,  9, 12,  4,  9,  5,  8,\n",
       "        13,  6, 12, 20,  7,  5, 10,  6, 13,  4,  6, 12,  9, 12, 14,  5,  9, 15,\n",
       "         7, 17,  4,  5, 16, 15, 20, 20, 12,  6, 10, 23, 18, 10,  5,  6, 15, 14,\n",
       "         7, 12, 23,  5, 11, 16, 20,  4,  9,  8, 20, 12, 15, 15, 14, 10, 14, 11,\n",
       "        10,  5,  9,  6,  8, 13,  7,  5, 18,  5,  7,  4, 13,  6,  5, 13, 23, 12,\n",
       "        20,  4,  8,  6,  9, 11,  5, 15,  6, 13, 19, 14,  4,  9,  5,  7, 10, 20,\n",
       "        16, 21,  4, 12,  5, 10,  9,  5,  9,  5,  5, 12, 19, 21,  4, 16,  4, 17,\n",
       "         9,  9,  4, 10, 10,  4,  5, 14, 12, 11,  8, 13, 14, 11,  9,  5, 11,  5,\n",
       "         7,  6,  5,  7,  9,  5,  8, 17, 15, 23, 23,  8,  6,  5, 12, 12,  7,  4,\n",
       "        11, 15,  8,  6, 10,  8,  5, 21, 16,  7,  5, 10, 19, 10, 14, 10,  5, 14,\n",
       "        12, 12,  5,  7, 11, 10, 18, 14, 16, 11,  5, 10, 16,  5, 21,  4, 19, 10,\n",
       "         6, 12, 17, 14,  7,  4, 23, 15, 13, 14,  7, 16,  9,  5, 22,  5,  9, 13,\n",
       "         7, 13,  4, 10,  7, 18, 17,  5, 20, 18,  7,  6, 15,  5, 10,  6, 17, 17,\n",
       "        15, 15,  6, 13,  7,  7, 12,  7,  4, 11,  6, 22, 10, 14,  6,  8,  6, 17,\n",
       "        11, 18, 11, 20, 10,  7,  7, 14,  7, 14,  2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sequence = \\\n",
    "\"\"\"\n",
    "MSKPHSEAGTAFIQTQQLHAAMADTFLEHMCRLDIDSPPITARNTGIICTIGPASRSVET\n",
    "LKEMIKSGMNVARLNFSHGTHEYHAETIKNVRTATESFASDPILYRPVAVALDTKGPEIR\n",
    "TGLIKGSGTAEVELKKGATLKITLDNAYMEKCDENILWLDYKNICKVVEVGSKIYVDDGL\n",
    "ISLQVKQKGADFLVTEVENGGSLGSKKGVNLPGAAVDLPAVSEKDIQDLKFGVEQDVDMV\n",
    "FASFIRKASDVHEVRKVLGEKGKNIKIISKIENHEGVRRFDEILEASDGIMVARGDLGIE\n",
    "IPAEKVFLAQKMMIGRCNRAGKPVICATQMLESMIKKPRPTRAEGSDVANAVLDGADCIM\n",
    "LSGETAKGDYPLEAVRMQHLIAREAEAAIYHLQLFEELRRLAPITSDPTEATAVGAVEAS\n",
    "FKCCSGAIIVLTKSGRSAHQVARYRPRAPIIAVTRNPQTARQAHLYRGIFPVLCKDPVQE\n",
    "AWAEDVDLRVNFAMNVGKARGFFKKGDVVIVLTGWRPGSGFTNTMRVVPVP\n",
    "\"\"\"\n",
    "\n",
    "def encode(sequence):\n",
    "    input_sequence = '<s> ' + ' '.replace('B', 'D').replace('Z', 'E').replace('J', 'L').join(sequence.replace('\\n', ''))\n",
    "    return roberta.task.source_dictionary.encode_line(input_sequence, add_if_not_exist=False)[:roberta.model.max_positions()].long()\n",
    "\n",
    "tokens = encode(example_sequence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_enzyme_features(tokens):\n",
    "    with torch.no_grad():\n",
    "        return roberta.extract_features(tokens).detach().cpu().numpy()[0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.11360448,  0.30674773, -0.01675967,  0.4423397 ,  0.01667783,\n",
       "       -0.18440159, -0.18239962,  0.2038919 , -0.5497034 ,  0.28601235],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_enzyme_features(tokens)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load swissprot sequences and annotations\n",
    "import pandas as pd\n",
    "import os\n",
    "swissprot_dir = '/projects/deepgreen/pstjohn/swissprot_go_annotation'\n",
    "\n",
    "swissprot = pd.read_parquet(os.path.join(swissprot_dir, 'parsed_swissprot_uniref_clusters.parquet'))\n",
    "go_terms = pd.read_parquet(os.path.join(swissprot_dir, 'swissprot_quickgo.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swissprot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_terms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ont = Ontology()\n",
    "ontology_data = pd.DataFrame(({'id': id_, **data} for id_, data in ont.G.nodes(data=True)))\n",
    "ontology_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This might be one way to get at NADH or NADPH dependent enzymes? But really we'll want pairs of enzymes\n",
    "ontology_data[ontology_data.name.str.contains('NADH')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
