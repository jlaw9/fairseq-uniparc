{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.models.roberta import RobertaModel\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from go_annotation import fairseq_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta = RobertaModel.from_pretrained('/gpfs/alpine/scratch/pstjohn/bie108/fairseq-uniparc/roberta_base_checkpoint/',\n",
    "                                       data_name_or_path='/ccs/home/pstjohn/project_work/swissprot_go_annotation/fairseq_swissprot/input0',\n",
    "                                       checkpoint_file='roberta.base_with_go_bias.pt')\n",
    "_ = roberta.eval()  # disable dropout (or leave in train mode to finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/ccs/home/pstjohn/project_work/swissprot_go_annotation/fairseq_swissprot_debug/input0/train.raw') as f:\n",
    "    input0 = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "\n",
    "targets = scipy.sparse.load_npz('/ccs/home/pstjohn/project_work/swissprot_go_annotation/fairseq_swissprot_debug/label/train.npz')[0]\n",
    "targets = torch.tensor(np.asarray(targets.todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20,  4,  9,  8,  4, 11,  4, 16, 14, 12,  5,  4,  7, 18,  6, 11,  7, 18,\n",
       "         4, 14,  6,  8, 15,  8,  7,  8, 18, 10,  5,  4,  4,  4,  5,  5,  4,  5,\n",
       "         9,  6, 11, 11, 16,  4, 18, 18,  7,  4, 13,  8, 13, 13, 12, 10, 21, 20,\n",
       "         4, 18,  5,  4, 16,  5,  4,  6,  7, 18, 17, 10,  4,  8,  5, 13, 10, 11,\n",
       "        23, 23,  9,  7, 13,  6,  4,  6,  6, 15,  4,  7,  5,  9, 16, 14,  4,  8,\n",
       "         4, 17,  4,  6, 18,  5,  6, 11,  5, 20, 10, 14,  4,  5,  5,  7,  4, 23,\n",
       "         4,  6, 18,  8, 13, 12,  7,  4, 11,  6,  9, 14, 10, 20, 15,  9, 10, 14,\n",
       "        12,  6, 21,  4,  7, 13,  5,  4, 10, 16,  6,  6,  5, 16, 12, 13, 19,  4,\n",
       "         9, 16,  9, 18, 19, 14, 14,  4, 10,  4, 10,  6,  6, 17, 10,  6,  6,  9,\n",
       "         4, 11,  7, 13,  6, 10,  7,  8,  8, 16, 17,  4, 11,  5,  4,  4, 20, 11,\n",
       "         5, 14,  4,  5,  9, 16, 13, 11, 11, 12, 10, 12, 20,  6, 13,  4,  7,  8,\n",
       "        15, 14, 19, 12, 13, 12, 11,  4, 21,  4, 20, 15,  5, 17,  6, 12, 13,  7,\n",
       "         6, 21,  9, 18, 19, 16, 12, 17, 21, 12, 15,  6,  6, 16, 11, 19, 10,  8,\n",
       "        14,  6, 11, 19,  4,  7,  9,  6, 13,  5,  8,  8,  5,  8, 19, 17,  4,  5,\n",
       "         5,  5,  5, 12, 15,  6,  6, 11,  7, 10,  7, 11,  6, 12,  6, 15, 15,  8,\n",
       "         7, 16,  6, 13, 11, 15, 17,  5, 13,  7,  4,  9, 15, 20,  6,  5, 15,  7,\n",
       "        11, 22,  6, 13, 13, 19, 12,  9, 23,  8, 10,  6,  9,  4, 16,  6, 12, 13,\n",
       "        20, 13, 20, 18, 21, 12, 14, 13,  5,  5, 20, 11, 12,  5, 11, 11,  5,  4,\n",
       "        17,  5, 11,  6, 14, 11, 11, 12, 10, 18, 12, 19, 18, 22, 10,  7, 15,  9,\n",
       "        11, 13, 10,  4, 11,  5, 20,  5, 11,  9,  4, 10, 15,  7,  6,  5,  9,  7,\n",
       "         9,  9,  6,  9, 13, 19, 12, 10,  7,  7, 14, 14,  7, 16,  4, 11,  5,  5,\n",
       "        13, 12,  6, 11, 19, 13, 13, 21, 10, 20,  5, 20, 23, 17,  8,  4,  7,  5,\n",
       "         4,  8, 13, 11, 14,  7, 11, 12,  4, 13, 14, 15, 23, 11,  5, 15, 11, 17,\n",
       "        14, 13, 19, 17,  9, 16, 17,  5, 10,  4,  8, 16,  4,  5,  2],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode(sequence):\n",
    "    return roberta.task.source_dictionary.encode_line(sequence)\n",
    "\n",
    "tokens = encode(input0)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = roberta.model(tokens.unsqueeze(0).long(), classification_head_name='go_prediction')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from go_annotation.ontology import Ontology\n",
    "ont = Ontology()\n",
    "\n",
    "_ancestor_array = ont.ancestor_array()\n",
    "\n",
    "import torch\n",
    "\n",
    "bsz = logits.shape[0]\n",
    "index_tensor = logits.new_tensor(_ancestor_array, dtype=torch.int64)\n",
    "index_tensor = index_tensor.unsqueeze(0).expand((bsz, -1, -1))  # Array of ancestors, offset by one\n",
    "padded_logits = torch.nn.functional.pad(logits, (1, 0), value=float('inf'))  # Make 0 index return inf\n",
    "padded_logits = padded_logits.unsqueeze(-1).expand((-1, -1, index_tensor.shape[2]))\n",
    "normed_logits = torch.gather(padded_logits, 1, index_tensor)\n",
    "normed_logits, _ = torch.min(normed_logits, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -9.0975,  -8.1519,  -7.9774,  ..., -13.1126, -13.2209, -13.3508]],\n",
       "       grad_fn=<MinBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6472, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed_logits.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32012])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32012])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "out = F.binary_cross_entropy_with_logits(normed_logits, targets, reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 1, 32012), dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normed_logits.detach().numpy()[np.isnan(out.detach().numpy())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(113.3611, dtype=torch.float64,\n",
       "       grad_fn=<BinaryCrossEntropyWithLogitsBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
