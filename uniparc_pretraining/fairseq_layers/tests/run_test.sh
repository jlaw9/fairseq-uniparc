fairseq-train fairseq_swissprot_debug \
	--user-dir ../../../uniparc_pretraining/ \
	--arch roberta_base \
	--task masked_lm_bias \
	--criterion masked_lm \
	--batch-size 2 \
	--untie-weights-roberta \
	--sample-break-mode complete \
	--total-num-update 10 \
	--max-positions 512 \
 	--shorten-method='random_crop' \
	--reset-optimizer --reset-dataloader --reset-meters \
	--weight-decay 0.1 --optimizer adam --adam-betas "(0.9, 0.98)" --adam-eps 1e-06 \
	--clip-norm 0.0 \
	--lr-scheduler polynomial_decay --lr 1e-05 \
	--dropout 0.1 --attention-dropout 0.1 \
	--save-interval 1 \
	--log-format simple --log-interval 1
